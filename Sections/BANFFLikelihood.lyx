#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
% ams math packages
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

% graphics packages
\usepackage{graphicx} % remove pdftex if you are not compiling to pdf
%\graphicspath{{./figures/}} % this places all graphics in the figures subdirectory

% allowed graphics extensions
% uncomment if you prefer to add extension in \includegraphics
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% allows the creation of subfigures
\usepackage[caption=false]{subfig}

% book tables are simple and look nice
\usepackage{booktabs}

% for specifying urls and links
\usepackage{url}
\urlstyle{same} % same style as regular text

% for defining colors
\usepackage{xcolor}

%\usepackage[labelformat=empty]{caption}
%\usepackage{subcaption}
\usepackage{lmodern}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{hepunits}
\usepackage{hepnames}
\usepackage{physics}

\usepackage[%
  pdfpagelabels,
  pdfusetitle,
  %hidelinks,
  %colorlinks=true,
  %pdfborder={0 0 0},
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=cyan,
  pagebackref,
  bookmarksopen,
  bookmarksnumbered]{hyperref}

\usepackage{sectsty}
\chapterfont{\centering}

\usepackage{braket}
\end_preamble
\use_default_options true
\master ../TN.lyx
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\pdf_title "HoganThesis"
\pdf_author "Matthew Hogan"
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize letterpaper
\use_geometry true
\use_package amsmath 0
\use_package amssymb 0
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\branch Introduction
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Subsection
BANFF Treatment of ND Constraint
\end_layout

\begin_layout Standard
The BANFF implementation aims to reduce the dimensionality, and hence complexity
, of the joint near detector (ND) and far detector (FD) problem by performing
 a separate analysis on the nuisance parameters that only the ND can measure.
 In a joint ND and FD joint fit, the measurements from both detectors are
 considered along with their respective systematic uncertainties.
 This approach is computationally expensive since the time to perform a
 fit increases non-linearly with dimensionality.
 BANFF considers a ND-only fit in order to decrease the computational demands.
 The BANFF post-fit parameters and their covariances are then propagated
 to the oscillation analysis using FD-only data.
 This allows for more rapidly completed studies on the effects of model
 parameters and biases present.
 Conceptually this approach should provide the same result with a joint
 ND and FD analysis.
 However, information encoded in the ND measurements for shared nuisance
 parameters is inevitably lost in this 
\begin_inset Quotes eld
\end_inset

divide-and-conquer
\begin_inset Quotes erd
\end_inset

 approach.
\end_layout

\begin_layout Standard
The BANFF ND-only constraint between 2015 through 2018 is described in detail
 in T2K-TN-220
\begin_inset CommandInset citation
LatexCommand citet
key "Hartz2015"
literal "false"

\end_inset

.
 While subsequent updates to the BANFF analysis increase the sample sizes
 and systematic parameterizations, the method has remained unchanged.
 It uses a frequentist approach to find the best nuisance parameter set
 to maximize a binned likelihood.
 The sets of nuisance, also called systematic, parameters in BANFF are
\end_layout

\begin_layout Itemize
cross section physics model parameters hereby denoted by 
\begin_inset Formula $\xsec$
\end_inset

,
\end_layout

\begin_layout Itemize
neutrino flux binned in neutrino energy hereby denoted by 
\begin_inset Formula $\flux$
\end_inset

, and
\end_layout

\begin_layout Itemize
detector systematics response weights that affect detector observables hereby
 denoted by 
\begin_inset Formula $\systematics$
\end_inset

.
\end_layout

\begin_layout Standard

\shape italic
A priori
\shape default
 (prior) knowledge of 
\begin_inset Formula $\xsec$
\end_inset

 and 
\begin_inset Formula $\flux$
\end_inset

 are available from external measurements and experiments.
 The 
\begin_inset Formula $\systematics$
\end_inset

 parameters are penalty terms whose effect can be understood and mitigated
 with control samples.
\end_layout

\begin_layout Subsubsection
BANFF Likelihood
\end_layout

\begin_layout Standard
In practice maximizing the log-likelihood (LLH) is performed instead of
 maximizing the likelihood function.
 In addition, multiplying the LLH by -2 turns the maximization problem into
 a minimization one which programs like MINUIT are designed to find.
 For an introduction into the use of likelihood functions in fits to histograms,
 see 
\begin_inset CommandInset citation
LatexCommand cite
key "baker-cousins"
literal "false"

\end_inset

 and the PDG review on Statistics.
\end_layout

\begin_layout Standard
To obtain the best set of parameters 
\begin_inset Formula $\xsec$
\end_inset

 and 
\begin_inset Formula $\flux$
\end_inset

 from the ND280 data that minimize the 
\begin_inset Formula $-2\times$
\end_inset

LLH, we need to predict how they affect our detector observables.
 Consider binned samples that select different charged current topologies.
 A convenient choice of observables for all the samples are the outgoing
 charged lepton 
\begin_inset Formula $l$
\end_inset

 momentum 
\begin_inset Formula $P_{l}$
\end_inset

 and angle 
\begin_inset Formula $\cos\theta_{l}$
\end_inset

 as mesaured in the ND since all the nuisance parameters affect these quantities.
 BANFF also uses an event-by-event weighting scheme to rapidly vary the
 event weights each of the cross section parameters.
 
\end_layout

\begin_layout Standard
We have the pieces needed to define the likelihood function used in ND-only
 BANFF analysis.
 Much of this is also documented in T2K-TN-220
\begin_inset CommandInset citation
LatexCommand cite
key "Hartz2015"
literal "false"

\end_inset

 where additional details can be found.
 For each 
\begin_inset Formula $\left(P_{l},\cos\theta_{l}\right)$
\end_inset

 analysis bin 
\begin_inset Formula $i=1,2,\ldots,M-1,M$
\end_inset

, the likelihood for the ND-constraint, 
\begin_inset Formula $\Lambda_{\ND280}^{r}$
\end_inset

, is a ratio of the predicted events,
\begin_inset Formula $\text{\vec{N}^{p}}$
\end_inset

, likelihood to the likelihood of the data events, 
\begin_inset Formula $\vec{N}^{d}$
\end_inset

, given as
\begin_inset Formula 
\begin{equation}
\begin{aligned}\Lambda_{\ND280}^{r} & =\frac{\mathcal{L}\left(\vec{N}^{p}\left(\xsec,\flux,\vec{r}\right)\left|\vec{N}^{d}\right.\right)}{\mathcal{L}\left(\vec{N}^{d}\left(\xsec,\flux,\vec{r}\right)\left|\vec{N}^{d}\right.\right)}\\
 & =\left(\prod_{\vec{y}=\xsec,\flux,\vec{r}}\frac{\pi\left(\vec{y}\right)}{\pi\left(\vec{y}_{\text{Nom}}\right)}\right)\left(\prod_{i=1}^{M}\left(\vec{N}_{i}^{p}\right)^{\vec{N}_{i}^{d}}\frac{e^{-\vec{N}_{i}^{p}}}{\vec{N}_{i}^{d}!}\right)\left(\prod_{j=1}^{M}\left(\vec{N}_{i}^{d}\right)^{\vec{N}_{i}^{d}}\frac{e^{-\vec{N}_{i}^{d}}}{\vec{N}_{i}^{d}!}\right)^{-1}
\end{aligned}
\label{eq:likelihoodratioND280}
\end{equation}

\end_inset

where constant terms have been dropped for brevity.
 Each of the prior distributions 
\begin_inset Formula $\pi\left(\vec{y}=\xsec,\flux,\systematics\right)$
\end_inset

 are assumed as Gaussians
\begin_inset Formula 
\begin{equation}
\pi(\vec{y})=\left(\frac{1}{\left(2\pi\right)^{k}\det\left(V_{y}\right)}\right)^{\frac{1}{2}}e^{\left(-\frac{1}{2}\Delta\vec{y}\cdot V_{y}^{-1}\cdot\Delta\vec{y}^{T}\right)},\label{eq:nuisancepriorgaussian-1}
\end{equation}

\end_inset

with 
\begin_inset Formula $V_{y}$
\end_inset

 being the covariance matrix for 
\begin_inset Formula $\vec{y}$
\end_inset

 and 
\begin_inset Formula $\Delta\vec{y}=\vec{y}-\vec{y}_{\text{Nominal}}$
\end_inset

 is the difference between the current and nominal set of vector parameters.
 When taking 
\begin_inset Formula $-2\log$
\end_inset

 of both sides, we use Wilks' theorem to define a chi-squared test statistic
 
\begin_inset Formula $\Delta\chi_{\ND280}^{2}=-2\log\Lambda_{\ND280}^{r}$
\end_inset

 which is given by
\begin_inset Formula 
\begin{equation}
\begin{aligned}\Delta\chi_{\ND280}^{2} & =2\sum_{i=1}^{M}\left[\vec{N}_{i}^{p}\left(\xsec,\flux,\vec{r}\right)-\vec{N}_{i}^{d}+\vec{N}_{i}^{d}\log\left(\frac{\vec{N}_{i}^{d}}{\vec{N}_{i}^{p}\left(\xsec,\flux,\systematics\right)}\right)\right]\\
 & +\Delta\xsec\cdot V_{x}^{-1}\cdot\left(\Delta\xsec\right)^{T}+\Delta\flux\cdot V_{b}^{-1}\cdot\left(\Delta\flux\right)^{T}+\Delta\vec{r}\cdot V_{r}^{-1}\cdot\left(\Delta\vec{r}\right)^{T}
\end{aligned}
\label{eq:BANFFDeltaChiSqr}
\end{equation}

\end_inset

where again constants were dropped from before.
 Since 
\begin_inset Formula $\vec{N}_{i}^{p}$
\end_inset

 is a function the nuisance parameters, this is calculated as
\begin_inset Formula 
\begin{equation}
\vec{N}_{i}^{p}\left(\xsec,\flux,\vec{r}\right)=w_{i}^{\text{POT}}\systematics_{i}\sum_{j=1}^{N_{i}^{\text{MC}}}\left[\sum_{k=1}^{N^{\text{Flux}}}\left(\delta_{j,k}^{\text{Flux}}\flux_{k}\right)\prod_{l=1}^{N^{\text{Syst}}}w_{j,l}\left(x_{l}^{\text{xsec}}\right)\right].\label{eq:n-predicted-events}
\end{equation}

\end_inset

Here 
\begin_inset Formula $w_{i}^{\text{POT}}$
\end_inset

 is the the ratio of of the number of true to simulated (MC) protons on
 target (POT) and 
\begin_inset Formula $N_{i}^{\text{MC}}$
\end_inset

 is the number of events in the 
\begin_inset Formula $i$
\end_inset

th analysis bin.
 The 
\begin_inset Formula $\systematics_{i}$
\end_inset

 parameters are observable normalization systematic parameters and vary
 the total number of predicted events in the analysis bin.
 The 
\begin_inset Formula $\flux_{k}$
\end_inset

 parmaeters, out of a total of 
\begin_inset Formula $N^{\text{Flux}}$
\end_inset

, are flux normalization systematics for each flux bin.
 Since the flux bins are categorized by neutrino flavor, energy, and horn
 (focusing magnet) current, the 
\begin_inset Formula $\delta_{j,k}^{\text{Flux}}$
\end_inset

 term selects the correct flux bin.
 The 
\begin_inset Formula $w_{j,l}\left(x_{l}^{\text{xsec}}\right)$
\end_inset

 parameters are pre-calculated event weight functions for each cross section
 (xsec) model parameter, 
\begin_inset Formula $x_{l}^{\text{xsec}}$
\end_inset

, out of a total of 
\begin_inset Formula $N^{\text{Syst}}$
\end_inset

 cross section systematics.
\end_layout

\begin_layout Standard
What follows here is a further description of the systematics and their
 implementation.
\end_layout

\begin_layout Subsubsection
Flux, Cross Section, and Detector Systematics
\end_layout

\begin_layout Standard
The minimization of the LLH is attempted by exploring the multidimensional
 parameter space to as best as possible match the data and prediction.
 To understand the response of each systematic, a covariance matrix is built
 for each source.
 A combined covariance matrix is used in used the BANFF analysis where each
 submatrix is initially uncorrelated with each other.
 The parameters in the covariance matrix are usually given a nominal value
 of one (1) unless a special value is needed with an uncertainty extracted
 using the Cholesky decomposition method.
 The development of each systematic uncertainty is briefly discussed below.
\end_layout

\begin_layout Standard

\series bold
Flux
\series default
: The flux weight is binned as a function of neutrino energy 
\begin_inset Formula $E_{\nu}$
\end_inset

 and divided by horn current (RHC or FHC) and neutrino flavor (
\begin_inset Formula $\numu$
\end_inset

, 
\begin_inset Formula $\numubar$
\end_inset

, 
\begin_inset Formula $\nue$
\end_inset

, and 
\begin_inset Formula $\nuebar$
\end_inset

).
 Each flux bin has a preset width with parameter that describes the weight
 and its uncertainty as shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:BANFF-ND280-flux"
plural "false"
caps "true"
noprefix "false"

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Figures/Introduction/FluxParametersPrePostFitTN324.png
	lyxscale 50
	width 75text%
	height 49theight%
	keepAspectRatio

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
BANFF ND280 flux 
\begin_inset Formula $\numu$
\end_inset

 and 
\begin_inset Formula $\numubar$
\end_inset

 binning parameters from T2K-TN-324 data post-fit results.
 The uncertainties are extracted from the pre-fit and post-fit covariance
 matrices.
\begin_inset CommandInset label
LatexCommand label
name "fig:BANFF-ND280-flux"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
BANFF ND280 NuMu and ANuMu Flux Binning Parameters
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

.
 Each parameter has a nominal value of one (1) and an increase of 10% (1.1)
 indicates the corresponding bin increases by 10% also.
 There are 50 ND and 50 SK parameters with a flux covariance matrix is shown
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:BANFF-pre-fit-flux-covariance"
plural "false"
caps "true"
noprefix "false"

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Figures/Introduction/BANFF_Flux_matrix.png
	lyxscale 50
	width 49text%
	height 49theight%
	keepAspectRatio

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
BANFF pre-fit flux covariance matrix shown with respective detector, horn
 current, and neutrino flavor.
\begin_inset CommandInset label
LatexCommand label
name "fig:BANFF-pre-fit-flux-covariance"

\end_inset


\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
BANFF Pre-fit Flux Covariance Matrix
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Cross Section
\series default
: There are a number of cross section models and weight functions implemented
 in BANFF.
 The cross section model used in this analysis is the 2017 NIWG parameterization.
 A technical description of the 2017 parameterization is given in T2K-TN-315
\begin_inset CommandInset citation
LatexCommand cite
key "Bolognesi2017"
literal "false"

\end_inset

 and T2K-TN-307
\begin_inset CommandInset citation
LatexCommand cite
key "Mahn2017"
literal "false"

\end_inset

.
 In general, there are model parameters that describe CC-0
\begin_inset Formula $\pion$
\end_inset

, CC-1
\begin_inset Formula $\pion$
\end_inset

, FSI (FSI: final state interactions, and smaller T2K effects.
 There are 25 cross section parameters as shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Cross-section-correlations-prefit"
plural "false"
caps "true"
noprefix "false"

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Figures/Introduction/BANFFXsecPrefitCorrelation.png
	lyxscale 50
	width 49text%
	height 25theight%
	keepAspectRatio

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cross section parameters pre-fit correlation matrix from the 2017 BANFF
 analysis.
\begin_inset CommandInset label
LatexCommand label
name "fig:Cross-section-correlations-prefit"

\end_inset


\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
Cross Section Parameters Pre-fit Correlation Matrix
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Bienstock2017"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard

\series bold
Detector Systematics
\series default
: Detector systematics are implemented as normalization changes to event
 kinematics as well as sample migration.
 In order to understand the how the detector systematics affect analysis
 bins, BANFF employs what are called observable normalization parameters,
 also commonly refered to as obsnorms.
 Since neutrino interaction events can migrate from sample-to-sample, bin-to-bin
, or both depending on the relevant systematics, numerous toy experiments
 are performed by varying systematic model parameters known to affect event
 or bin migration.
 After many toy experiments, usually ~2000, the mean values and covariances
 are extracted from all the experiments over all observable normalization
 bins.
 Therefore the nominal value for each obsnorm parameter is not one (1).
 The number of obsnorm parameters are determined by the analyzer by defining
 a set of bins that can encapsulate how events migrate across bins.
 A detector covariance matrix is contructed using all toy experiments.
 The drawback to this method is that not all detector systematics have Gaussian
 responses to the observables, and so the correlations are not fully accurate.
\end_layout

\begin_layout Standard
Ideally there would be one normalization for each analysis bin.
 However due to computational and time limitations, a single observable
 normalization parameter are assigned to multiple analysis bins.
 
\end_layout

\begin_layout Subsection
Usage of ND280 Psyche Software
\end_layout

\begin_layout Standard
Psyche is a general framework for data handling, event selections, and systemati
c evaluations with toy experiments.
 Psyche is a 
\begin_inset Quotes eld
\end_inset

lean
\begin_inset Quotes erd
\end_inset

 package from the perspective of analyzing MC events since that functionality
 is built heavily into Highland2.
 The analysis performed in this technical note required making additions
 to psyche in order replicate features available in Highland2.
 It would be wise for future analyses to build a selection in Highland2
 and migrate that psyche once mature.
\end_layout

\begin_layout Standard
BANFF uses a psyche package called psycheSteering that interfaces with all
 the psyche tools to manage the migration of samples into its analysis code.
 New 
\begin_inset Formula $\pod$
\end_inset

 selections were added to the psycheSelections package and validated using
 the psycheSteering AnalysisManager class.
 The AnalysisManager provides the functionality to get the true and reconstructe
d detector observables from each reconstructed event along with the flux
 tunning and detector systematic weights.
\end_layout

\begin_layout Standard
Flux tunning is the process of applying an event weight based on the true
 neutrino energy, flavor, and run period.
 Since the ND280 MC uses a series of models to describe the expected neutrino
 flux, it cannot perfectly model the true flux nor know the beam conditions
 at run time.
 The beam group is responsible for releasing the expected and measured neutrino
 flux in order to account for these differences.
 To flux tune an event, the relevant neutrino flavor flux histogram must
 be referenced.
 The weight is extracted by taking the ratio of the tuned flux to the nominal
 flux in the MC for a given neutrino energy.
 As an example 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Figures/Introduction/FluxTunningnumuFHC.png
	lyxscale 50
	width 49text%
	height 25theight%
	keepAspectRatio

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Fluxing tuning histogram for 
\begin_inset Formula $\numu$
\end_inset

 FHC events taken from the 13av3 flux release.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Fluxing-tuning-histogram"

\end_inset


\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
Fluxing Tuning Histogram for FHC Events
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Fluxing-tuning-histogram"
plural "false"
caps "true"
noprefix "false"

\end_inset

 shows the flux tuning weights for true 
\begin_inset Formula $\numu$
\end_inset

 FHC events.
\end_layout

\end_body
\end_document
